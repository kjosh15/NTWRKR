MIME-Version: 1.0
Date: Tue, 22 Sep 2020 08:44:58 +0000
References: <003001d69028$1ae03080$50a09180$@indigometrics.com>
In-Reply-To: <003001d69028$1ae03080$50a09180$@indigometrics.com>
Message-ID: <CACNgykMhrxe_LqzToVrsoyRxbp0BenajaibPR8xd3SckykbA_A@mail.gmail.com>
Subject: Re: Job Matching Algo
From: Josh Klein <josh@josh.is>
To: Chris at Indigo <chris@indigometrics.com>
Content-Type: multipart/alternative; boundary="000000000000730e2205afe2fb7b"

--000000000000730e2205afe2fb7b
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Would love to hear more on switching the algo to AI rather than expert,
esp. in terms of libraries we could leverage plus integration points for
new services.

In terms of scalability, first thing that comes to mind is shifting it onto
the cloud, second is to build a rack in your shed. Let's talk about
time/cost and where we think we need scale first - at present your shift to
linear vs. exponential seems to have saved us!

best,
Josh
=E1=90=A7

On Mon, Sep 21, 2020 at 3:02 PM <chris@indigometrics.com> wrote:

> J,
>
> I have the job matching algo down to about 3 seconds per participant on a
> slowish PC with linear expansion as the org gets bigger (this is the big
> change =E2=80=93 used to be exponential).
>
> So 10,000 participants takes 8 hours to calculate on my old PC =E2=80=93 =
guess it
> will keep us going for a few months and by speccing up the virtual machin=
e
> we could run 50,000 in about the same time.
>
> But we=E2=80=99ll (hopefully =E2=80=93 given volumes) need to both redo m=
y code using a
> professional and switch the algo to AI rather than expert quite soon=E2=
=80=A6=E2=80=A6=E2=80=A6=E2=80=A6
> I=E2=80=99m starting to get ideas about how we could use AI to solve the =
problem so
> maybe that will come to our rescue.
>
> This is the most complicated algo we have, the other pinch point is
> calculating the historic graphs for every participant and team node =E2=
=80=93
> currently takes about 1 hour per 1000 employees on a slowish PC with most=
ly
> linear expansion =E2=80=93 but then again we can just leave it running an=
d it will
> be done in about 2 days even for 50,000 participants =E2=80=93 and we onl=
y update
> the graphs weekly in any case.
>
> C
>

--000000000000730e2205afe2fb7b
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Would love to hear more on switching the algo to AI rather=
 than expert, esp. in terms of libraries we could leverage plus integration=
 points for new services.=C2=A0<div><br></div><div>In terms of scalability,=
 first thing that comes to mind is shifting it onto the cloud, second is to=
 build a rack in your shed. Let&#39;s talk about time/cost and where we thi=
nk we need scale first - at present your shift to linear vs. exponential se=
ems to have saved us!</div><div><br></div><div>best,</div><div>Josh</div></=
div><div hspace=3D"streak-pt-mark" style=3D"max-height:1px"><img alt=3D"" s=
tyle=3D"width:0px;max-height:0px;overflow:hidden" src=3D"https://mailfoogae=
.appspot.com/t?sender=3Daam9zaEBqb3NoLmlz&amp;type=3Dzerocontent&amp;guid=
=3D745ff34c-6b16-49e1-a691-be3f44ee333a"><font color=3D"#ffffff" size=3D"1"=
>=E1=90=A7</font></div><br><div class=3D"gmail_quote"><div dir=3D"ltr" clas=
s=3D"gmail_attr">On Mon, Sep 21, 2020 at 3:02 PM &lt;<a href=3D"mailto:chri=
s@indigometrics.com">chris@indigometrics.com</a>&gt; wrote:<br></div><block=
quote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1=
px solid rgb(204,204,204);padding-left:1ex"><div lang=3D"EN-GB"><div class=
=3D"gmail-m_1189914116274402094WordSection1"><p class=3D"MsoNormal">J,<u></=
u><u></u></p><p class=3D"MsoNormal">I have the job matching algo down to ab=
out 3 seconds per participant on a slowish PC with linear expansion as the =
org gets bigger (this is the big change =E2=80=93 used to be exponential).<=
u></u><u></u></p><p class=3D"MsoNormal">So 10,000 participants takes 8 hour=
s to calculate on my old PC =E2=80=93 guess it will keep us going for a few=
 months and by speccing up the virtual machine we could run 50,000 in about=
 the same time.<u></u><u></u></p><p class=3D"MsoNormal">But we=E2=80=99ll (=
hopefully =E2=80=93 given volumes) need to both redo my code using a profes=
sional and switch the algo to AI rather than expert quite soon=E2=80=A6=E2=
=80=A6=E2=80=A6=E2=80=A6 I=E2=80=99m starting to get ideas about how we cou=
ld use AI to solve the problem so maybe that will come to our rescue.<u></u=
><u></u></p><p class=3D"MsoNormal">This is the most complicated algo we hav=
e, the other pinch point is calculating the historic graphs for every parti=
cipant and team node =E2=80=93 currently takes about 1 hour per 1000 employ=
ees on a slowish PC with mostly linear expansion =E2=80=93 but then again w=
e can just leave it running and it will be done in about 2 days even for 50=
,000 participants =E2=80=93 and we only update the graphs weekly in any cas=
e. <u></u><u></u></p><p class=3D"MsoNormal">C<u></u><u></u></p></div></div>=
</blockquote></div>

--000000000000730e2205afe2fb7b--