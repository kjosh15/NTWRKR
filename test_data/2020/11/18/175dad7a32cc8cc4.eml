MIME-Version: 1.0
Date: Wed, 18 Nov 2020 10:12:46 +0000
Message-ID: <CACNgykNr4Asz5xK93+HEFwA6fdfQwL0csSrkoMXk3MoGG_aM7A@mail.gmail.com>
Subject: GPT-3 for newswires
From: Josh Klein <josh@josh.is>
To: Matthew Danzico <matt@mattdanzico.com>
Content-Type: multipart/alternative; boundary="0000000000005128df05b45eda7a"

--0000000000005128df05b45eda7a
Content-Type: text/plain; charset="UTF-8"

As expected, the answer to this is complicated. To start, though, the folks 
that made GPT-3 wrote an article about how to do it:

https://openai.com/blog/learning-to-summarize-with-human-feedback/

Here's an open-source codebase for doing the same thing using GPT-2 (free 
library):

https://blog.paperspace.com/generating-text-summaries-gpt-2/

Note that this also has notes on how to correct it. In other words, you'd 
want to hire someone who knows about GPT-2/3 and language models to work on 
the project and set up the training loop to use human beings to 
correct/spot-check the results in order to improve it over time. You'd also 
want to throw some $$ at it to support big processing capabilities in order 
to scale and improve it quickly. That said, summarizing news wires seems 
like a totally viable implementation as long as you have human beings 
reviewing/approving the summaries (at least until the model was trained 
well enough to avoid miss

--0000000000005128df05b45eda7a
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">As expected, the answer to this is complicated. To start, =
though, the folks that made GPT-3 wrote an article about how to do it:<div>=
<br></div><div><a href=3D"https://openai.com/blog/learning-to-summarize-wit=
h-human-feedback/">https://openai.com/blog/learning-to-summarize-with-human=
-feedback/</a><br></div><div><br></div><div>Here&#39;s an open-source codeb=
ase for doing the same thing using GPT-2 (free library):</div><div><br></di=
v><div><a href=3D"https://blog.paperspace.com/generating-text-summaries-gpt=
-2/">https://blog.paperspace.com/generating-text-summaries-gpt-2/</a><br></=
div><div><br></div><div>Note that this also has notes on how to correct it.=
 In other words, you&#39;d want to hire someone who knows about GPT-2/3 and=
 language models to work on the project and set up the training loop to use=
 human beings to correct/spot-check the results in order to improve it over=
 time. You&#39;d also want to throw some $$ at it to support big processing=
 capabilities in order to scale and improve it quickly. That said, summariz=
ing news wires seems like a totally viable implementation as long as you ha=
ve human=C2=A0beings reviewing/approving the summaries (at least until the =
model was trained well enough to avoid miss</div></div>

--0000000000005128df05b45eda7a--